# Experiment: Model Routing

## Introduction

In this small project I experimented with the model routing architecture
and potential implementation paths. In my opinion, a model router is almost
always required in production systems, due to its tremendous benefits.
In multimodal systems, it is a core element. Even model architectures like
Mixture-of-experts use this concept to optimize training and inference cost.
Routing is by its nature a categorization task, which LLMs have proven to be good at.
This provides major flexibility in the router model choice, especially regarding the model size.
Comparably small models (2-3b) can deliver reliable results when paired with accurate fault tolerance techniques. 

### Benefits

1. Reduction of computational cost, since the bigger model is inferenced less often
2. Partial Loadbalancing
3. Potential fallback mechanism
4. Extensibility
5. Improved User Experience (Bigger models tend to give too long or too complicated responses)

The benefits of this approach are its simplicity, and the potential for several
system optimizations. 

For example, depending on the system, this approach could be used to generate a specialized complexity
routing where the data for the threshold estimation is based on captured queries and therefore may
improve the overall accuracy, because it basically reduces the solution space to environment specific needs.

## Development

### Core Idea

My favourite area in LLM-Programming is inference-time optimization. 
I experiment with prompts, architectures and workflows to learn how to use
Large Language Models most effectively.

Here I explore a threshold based approach, routing the query according to its
estimated complexity. 

The complexity estimation is an experimental approach where I combined different
frameworks to generate a dataset that is further processed using simple mathematical correlations 
which results in a final threshold score. 

State-of-the-art techniques I used:

1. Hallucination reduction - Model Sampling & Mean Score

    A well known technique is model sampling. It is the core of concepts like "majority voting"
    where the final response is generated by sampling multiple answers to the same query
    and agree on the best of these via dedicated mechanism. In the case of majority voting
    the answers are rated by other LLMs or similiar.

    I replaced the voting mechanism by using mean aggregation. Since the valid responses are in a numerical solution space, the impact of outliers (hallucinations) can be reduced
    by taking the mean of the sampled responses, due to the flattening effect of this operation.
    The upper and lower bound (indicating the environment complexity and if the model is sufficient for it)
    are simply derived by using relations between the min/max scores from each category (easy, hard). Both are used to calculate
    the midpoint, taking uncovered ranges into consideration.

    The output of the router is a simple binary decision, which indicates that accuracy is not as important
    and therefore supports the decision for mean aggregation.

2. Self-assessment

    The model that generates the test set responses for the router threshold approximation 
    should also be the weaker system model. This creates a probabilistic framework 
    which increases the routing reliability by reducing the chances of 
    incorrectly routing too complex queries to the simpler model.

Note on Hallucinations:

Models implicitly express their limitations through hallucinations rather than direct refusals when faced with queries they cannot properly address.
The challenge lies not just in detecting these hallucinations, but in managing their cascading effects in context-sensitive environments.

For example, in a conversational AI system, the hallucination must be detected
immediately before the conversation proceeds. Once the system failed to detect and
replace the hallucinated response, the entire subsequent message history is corrupted.
The hallucinated response could potentially trigger an instruction-contradiction feedback loop, where:

1. The model's response contradicts its system prompt
2. The conversation continues without correction
3. The model interprets this continuation as validation

This reinforcement increases the likelihood of further hallucinations. The model attends to the implicit feedback "conversation goes on I must be right". It interprets the continued conversation as validation for its previous responses, if not criticized.

This dynamic warrants further research, particularly regarding how these error states influence the model's attention patterns during extended conversations.

## Data 

The evaluation data in the (data/data.json) is AI generated and not further engineered, but reviewed.
The generated data set (data/result.json) is created as described above.
During development I tried several approaches and other statistical measures, as some artifacts of this exploration may be found in the code.

## Process Flow

![process](resources/model-router.png)

## Evaluation & Validation

The routing quality can be evaluated using environment specific metrics:

1. Create test dataset 
2. Human-label the data (Labeling with SOTA models may be sufficient)
3. Run the router module using the test dataset as inputs and store the outputs
4. Calculate performance metrics like accuracy, precision, and recall for the binary routing decisions and false positive rate (incorrectly routing to complex model) vs. false negative rate (incorrectly routing to simple model).

Note: Don't forget to store the current prompt in your benchmark result.

The validation should be implemented in form of monitoring, continously capturing missrouted queries. This data could be used to train the router model periodically.
